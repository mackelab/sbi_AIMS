{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Conditional Density Estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from math import pi, log\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from torch.distributions import Normal\n",
    "import matplotlib as mpl\n",
    "\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Density estimation\n",
    "\n",
    "Density estimation means to estimate the probability distribution $p(\\theta)$ from samples $\\theta_1, \\theta_2, ...$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are many ways to do this:\n",
    "\n",
    "- kernel density estimation\n",
    "- non-parametric approaches with order statistics\n",
    "- many neural network methods (GANs, VAEs, Normalizing flows,...)\n",
    "- ...\n",
    "\n",
    "Here, we will discuss (conditional) density estimation with Mixtures of Gaussians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's move to a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean = 1.0\n",
    "std = 0.4\n",
    "\n",
    "normal_dist = Normal(mean, std)  # We do not usually know this...\n",
    "samples = normal_dist.sample((50,))  # ...but all we have are these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with mpl.rc_context(fname=\".matplotlibrc\"):\n",
    "    fig = plt.figure(figsize=(6, 2))\n",
    "    plt.plot(\n",
    "        samples,\n",
    "        np.zeros_like(samples),\n",
    "        \"bx\",\n",
    "        alpha=0.5,\n",
    "        markerfacecolor=\"none\",\n",
    "        markersize=6,\n",
    "    )\n",
    "    _ = plt.xlim([-4, 4])\n",
    "    _ = plt.ylim([-0.12, 1.2])\n",
    "    # plt.savefig(\"figures/samples_gauss.png\", dpi=400, bbox_inches=\"tight\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/samples_gauss.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "Our goal is to estimate the underlying distribution of these samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 1: fitting a Gaussian to data with maximum likelihood\n",
    "\n",
    "### Learning mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_Gauss(x, mean, std):\n",
    "    \"\"\"Gaussian probability density function\n",
    "    args:\n",
    "        mean: mean of the Gaussian\n",
    "        std: standard deviation of the Gaussian\n",
    "    returns:\n",
    "        probability density of a one dimensional Gaussian\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement the Gaussian probability density function\n",
    "\n",
    "    prob = (\n",
    "        1 / torch.sqrt(2 * pi * std**2) * torch.exp(-0.5 / std**2 * (x - mean) ** 2)\n",
    "    )\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(samples)\n",
    "train_loader = data.DataLoader(samples, batch_size=10)\n",
    "\n",
    "learned_mean = torch.nn.Parameter(torch.zeros(1))\n",
    "learned_log_std = torch.nn.Parameter(torch.zeros(1))\n",
    "\n",
    "opt = optim.Adam([learned_mean, learned_log_std], lr=0.005)\n",
    "\n",
    "for e in range(500):\n",
    "    for sample_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "        learned_std = torch.exp(learned_log_std)\n",
    "        prob = prob_Gauss(sample_batch, learned_mean, learned_std)\n",
    "        loss = -torch.log(prob).mean()\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Learned mean: \",\n",
    "    learned_mean.item(),\n",
    "    \", learned standard deviation: \",\n",
    "    torch.exp(learned_log_std).item(),\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"True mean: \",\n",
    "    mean,\n",
    "    \", true standard deviation: \",\n",
    "    std,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "true_dist = Normal(mean, std)\n",
    "learned_dist = Normal(learned_mean, torch.exp(learned_log_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(-4, 4, 100)\n",
    "true_probs = torch.exp(normal_dist.log_prob(x))\n",
    "learned_probs = torch.exp(learned_dist.log_prob(x)).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with mpl.rc_context(fname=\".matplotlibrc\"):\n",
    "    fig = plt.figure(figsize=(6, 2))\n",
    "    plt.plot(\n",
    "        samples,\n",
    "        np.zeros_like(samples),\n",
    "        \"bx\",\n",
    "        alpha=0.5,\n",
    "        markerfacecolor=\"none\",\n",
    "        markersize=6,\n",
    "    )\n",
    "    plt.plot(x, true_probs)\n",
    "    plt.plot(x, learned_probs)\n",
    "    plt.legend([\"Samples\", \"Ground truth\", \"Learned\"], loc=\"upper left\")\n",
    "    _ = plt.xlim([-4, 4])\n",
    "    _ = plt.ylim([-0.12, 1.2])\n",
    "    # plt.savefig(\"figures/fitted_samples.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/fitted_samples.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Questions to think about\n",
    "\n",
    "1) Why do we parameterize the logarithm of the standard deviation instead of the standard deviation itself?  \n",
    "2) We are evaluating the probability in batches of samples. Does this also work for single samples (i.e. non-batched)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Conditional density estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Let's look at a simple model\n",
    "Generate data from $x = \\theta + 0.3 \\sin(2 \\pi \\theta) + \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# generate data\n",
    "n = 4000  # number of datapoints\n",
    "d = 1  # dimensionality of parameters theta\n",
    "\n",
    "theta = torch.rand((n, d))\n",
    "noise_std = 0.05\n",
    "\n",
    "def generate_data(theta, noise_std):\n",
    "    noise = torch.randn(theta.shape) * noise_std\n",
    "    x = theta + 0.3 * torch.sin(2 * torch.pi * theta) + noise\n",
    "    return x\n",
    "\n",
    "x = generate_data(theta, noise_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# set the conditional value, in which we are interested in\n",
    "# p(. | x = val_to_eval)\n",
    "val_to_eval = torch.tensor(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Data t_train and x_train\n",
    "with mpl.rc_context(fname=\".matplotlibrc\"):\n",
    "    fig = plt.figure(figsize=(4.5, 2.2))\n",
    "    plt.plot(theta[:400], x[:400], \"go\", alpha=0.4, markerfacecolor=\"none\")\n",
    "    plt.axvline(val_to_eval.numpy())\n",
    "    plt.xlabel(r\"$\\theta$\")\n",
    "    plt.ylabel(\"x\")\n",
    "    # plt.savefig(\"figures/cde_samples.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Data t_train and x_train\n",
    "\n",
    "mean = val_to_eval + 0.3 * torch.sin(2 * pi * val_to_eval)\n",
    "\n",
    "with mpl.rc_context(fname=\".matplotlibrc\"):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4.5, 2.2))\n",
    "    x_vals = torch.linspace(-0.2, 1.0, 1000)\n",
    "    n = Normal(mean, 0.05)\n",
    "    y_vals = n.log_prob(x_vals).exp().numpy()\n",
    "    x_vals = x_vals.numpy()\n",
    "    plt.plot(x_vals, np.flip(y_vals), linewidth=2)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.set_ylabel(r\"$p(x|\\theta=0.1)$\", rotation=90, fontsize=18)\n",
    "    # plt.savefig(\"figures/conditional.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The goal is to estimate parameters of a conditional distribution:\n",
    "\n",
    "<img src=\"figures/cond_assembled.png\" alt=\"drawing\" width=\"1100\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/nn.png\" alt=\"drawing\" width=\"900\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "### Learning mean and standard deviation conditional on inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: implement the neural net as fully connected network and ReLU activation function\n",
    "# net = nn.Sequential(...)\n",
    "\n",
    "# Hint: what is the input dimensionality of the network? and the output dimensionality?\n",
    "# the output of the net should be the mean and the log_std of the Gaussian distribution.\n",
    "# one hidden layer with 20 neurons is sufficient. .\n",
    "\n",
    "# Solution:\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(1, 20), nn.ReLU(), nn.Linear(20, 20), nn.ReLU(), nn.Linear(20, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(theta, x)\n",
    "train_loader = data.DataLoader(dataset, batch_size=20)\n",
    "\n",
    "opt = optim.Adam(net.parameters(), lr=0.01)\n",
    "for e in range(50):\n",
    "    for theta_batch, x_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "        nn_output = net(theta_batch)\n",
    "        mean = nn_output[:, 0].unsqueeze(1)\n",
    "        std = torch.exp(nn_output[:, 1]).unsqueeze(1)\n",
    "        prob = prob_Gauss(x_batch, mean, std)\n",
    "        loss = -torch.log(prob).sum()\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We now have a neural network which outputs the parameters of a probability density distribution given inputs.\n",
    "In this case, it was a Gaussian distribution of which we learned the mean and standard deviation. To evaluate the result, we can look at samples from the returned distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inspect the distribution for t = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "theta_test = tensor([0.1])\n",
    "nn_output = net(theta_test)\n",
    "conditional_mean = nn_output[0].detach().numpy()\n",
    "conditional_std = torch.exp(nn_output[1]).detach().numpy()\n",
    "print(\n",
    "    \"Learned: Conditional mean: \",\n",
    "    conditional_mean,\n",
    "    \", conditional std: \",\n",
    "    conditional_std,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"True: Conditional mean: \",\n",
    "    generate_data(theta_test, noise_std=0.0).item(),\n",
    "    \", conditional std: \",\n",
    "    noise_std,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Data t_train and x_train\n",
    "with mpl.rc_context(fname=\".matplotlibrc\"):\n",
    "    fig = plt.figure(figsize=(4.5, 2.2))\n",
    "    plt.plot(theta[:400], x[:400], \"go\", alpha=0.4, markerfacecolor=\"none\", zorder=-100)\n",
    "    plt.plot(\n",
    "        [theta_test, theta_test],\n",
    "        [conditional_mean - conditional_std, conditional_mean + conditional_std],\n",
    "        c=\"r\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    plt.scatter(theta_test, conditional_mean, c=\"r\", s=30, alpha=1.0)\n",
    "    plt.xlabel(r\"$\\theta$\")\n",
    "    plt.ylabel(\"x\")\n",
    "    # plt.savefig(\"figures/cde_fitted.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/cde_fitted.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alternative evaluation: for every $t$, sample from the Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "theta_test = torch.linspace(0.0, 1.0, 500).unsqueeze(1)\n",
    "\n",
    "for single_theta in theta_test:\n",
    "    network_outs = net(single_theta.unsqueeze(1))\n",
    "    m = network_outs[:, 0]\n",
    "    s = torch.exp(network_outs[:, 1])\n",
    "    conditional_distribution = Normal(m, s)\n",
    "    sample = conditional_distribution.sample((1,))\n",
    "\n",
    "    samples.append(sample)\n",
    "samples = torch.cat(samples).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "with mpl.rc_context(fname=\".matplotlibrc\"):\n",
    "    fig = plt.figure(figsize=(4.5, 2.2))\n",
    "    plt.plot(theta[:400], x[:400], \"go\", alpha=0.5, markerfacecolor=\"none\")\n",
    "    plt.plot(theta_test, samples.squeeze().detach(), \"ro\", linewidth=3.0)\n",
    "    plt.xlabel(r\"$\\theta$\")\n",
    "    plt.ylabel(\"x\")\n",
    "    # plt.savefig(\"figures/cde_fitted_samples.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/cde_fitted_samples.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But what if the conditional distribution is not Gaussian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Again, generate data from $x = t + 0.3 \\sin(2 \\pi t) + \\epsilon$. \n",
    "\n",
    "But now, predict $\\theta$ from $x$, i.e. learn $p(\\theta | x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with mpl.rc_context(fname=\".matplotlibrc\"):\n",
    "    fig = plt.figure(figsize=(4.5, 2.2))\n",
    "    plt.plot(x[:400], theta[:400], \"go\", alpha=0.5, markerfacecolor=\"none\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(r\"$\\theta$\")\n",
    "    # plt.savefig(\"figures/cde_samples_inv.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/cde_samples_inv.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's train this just like before, but now predicting $\\theta$ from $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(x, theta)\n",
    "train_loader = data.DataLoader(dataset, batch_size=20)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(1, 20), nn.ReLU(), nn.Linear(20, 20), nn.ReLU(), nn.Linear(20, 2)\n",
    ")\n",
    "\n",
    "opt = optim.Adam(net.parameters(), lr=0.01)\n",
    "for e in range(100):\n",
    "    for x_batch, theta_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "        nn_output = net(x_batch)\n",
    "        mean = nn_output[:, 0].unsqueeze(1)\n",
    "        std = torch.exp(nn_output[:, 1]).unsqueeze(1)\n",
    "        prob = prob_Gauss(theta_batch, mean, std)\n",
    "        loss = -torch.log(prob).sum()\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from the learned conditional distribution\n",
    "\n",
    "x_test = torch.linspace(-0.1, 1.1, 500).unsqueeze(1)\n",
    "\n",
    "network_outs = net(x_test)\n",
    "mean = network_outs[:, 0]\n",
    "std = torch.exp(network_outs[:, 1])\n",
    "conditional_distribution = Normal(mean, std)\n",
    "sample = conditional_distribution.sample((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "with mpl.rc_context(fname=\".matplotlibrc\"):\n",
    "    fig = plt.figure(figsize=(4.5, 2.2))\n",
    "    plt.plot(x[:400], theta[:400], \"go\", alpha=0.5, markerfacecolor=\"none\")\n",
    "    plt.plot(x_test, samples.squeeze().detach(), \"ro\", linewidth=3.0)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(r\"$\\theta$\")\n",
    "    # plt.savefig(\"figures/cde_fitted_inv_samples.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/cde_fitted_inv_samples.png\" alt=\"drawing\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain this result?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Mixture Density Networks\n",
    "$$\n",
    "$$\n",
    "<img src=\"figures/architecture.png\" alt=\"drawing\" width=\"1200\"/>\n",
    "\n",
    "The loss is now the probability under this mixture of Gaussians  <br/>\n",
    "\n",
    "$\\mathcal{L} = -\\sum_i \\log q(\\theta_i | x_i) = -\\sum_i \\log \\sum_j \\alpha_{j,i} \\mathcal{N}(\\theta_i; \\mu_{j,i}(x_i), \\sigma_{j,i}(x_i)$  <br/>\n",
    "\n",
    "We learn the mixture components $\\alpha_j$, the means $\\mu_j$, and the variances $\\sigma_j$.\n",
    "\n",
    "At the moment we will restrict ourselves to diagonal covariance matrices, meaning we can not learn any correlation structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class MultivariateGaussianMDN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multivariate Gaussian MDN with diagonal Covariance matrix.\n",
    "\n",
    "    For a documented version of this code, see:\n",
    "    https://github.com/mackelab/pyknos/blob/main/pyknos/mdn/mdn.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features,\n",
    "        hidden_net,\n",
    "        num_components,\n",
    "        hidden_features,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._features = features\n",
    "        self._num_components = num_components\n",
    "\n",
    "        self._hidden_net = hidden_net\n",
    "        self._logits_layer = nn.Linear(hidden_features, num_components)\n",
    "        self._means_layer = nn.Linear(hidden_features, num_components * features)\n",
    "        self._unconstrained_diagonal_layer = nn.Linear(\n",
    "            hidden_features, num_components * features\n",
    "        )\n",
    "\n",
    "    def get_mixture_components(self, context):\n",
    "        h = self._hidden_net(context)\n",
    "\n",
    "        # mixture coefficients in log space\n",
    "        logits = self._logits_layer(h)\n",
    "        logits = logits - torch.logsumexp(logits, dim=1).unsqueeze(1)  # normalization\n",
    "\n",
    "        # means\n",
    "        means = self._means_layer(h).view(-1, self._num_components, self._features)\n",
    "\n",
    "        # log variances for diagonal Cov matrix\n",
    "        # otherwise: Cholesky decomposition s.t. Cov = AA^T, A is lower triangular.\n",
    "        log_variances = self._unconstrained_diagonal_layer(h).view(\n",
    "            -1, self._num_components, self._features\n",
    "        )\n",
    "        variances = torch.exp(log_variances)\n",
    "\n",
    "        return logits, means, variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def mog_log_prob(theta, logits, means, variances):\n",
    "    \"\"\" Log probability of a mixture of Gaussians.\n",
    "        args:\n",
    "            theta: parameters\n",
    "            logits: log mixture coefficients\n",
    "            means: means of the Gaussians\n",
    "            variances: variances of the Gaussians\n",
    "        returns:\n",
    "            log probability of the mixture of Gaussians\"\"\"\n",
    "    _, _, theta_dim = means.size()\n",
    "    theta = theta.view(-1, 1, theta_dim)\n",
    "\n",
    "    log_cov_det = -0.5 * torch.log(torch.prod(variances, dim=2))\n",
    "\n",
    "    a = logits\n",
    "    b = -(theta_dim / 2.0) * log(2 * pi)\n",
    "    c = log_cov_det\n",
    "    d1 = theta.expand_as(means) - means\n",
    "    precisions = 1.0 / variances\n",
    "    exponent = torch.sum(d1 * precisions * d1, dim=2)\n",
    "    exponent = tensor(-0.5) * exponent\n",
    "\n",
    "    return torch.logsumexp(a + b + c + exponent, dim=-1)\n",
    "\n",
    "\n",
    "def mog_sample(logits, means, variances):\n",
    "    \"\"\"Sample from a mixture of Gaussians.\n",
    "        args:\n",
    "            logits: log mixture coefficients\n",
    "            means: means of the Gaussians\n",
    "            variances: variances of the Gaussians   \n",
    "        returns:\n",
    "            samples from the mixture of Gaussians\"\"\"\n",
    "    \n",
    "    coefficients = F.softmax(logits, dim=-1)\n",
    "    choices = torch.multinomial(coefficients, num_samples=1, replacement=True).view(-1)\n",
    "    chosen_means = means[0, choices, :]  # 0 for first batch position\n",
    "    chosen_variances = variances[0, choices, :]\n",
    "\n",
    "    _, _, output_dim = means.shape\n",
    "    standard_normal_samples = torch.randn(output_dim)\n",
    "    zero_mean_samples = standard_normal_samples * torch.sqrt(chosen_variances)\n",
    "    samples = chosen_means + zero_mean_samples\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(x, theta)\n",
    "train_loader = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=50,\n",
    ")\n",
    "\n",
    "# TODO: implement the neural net as fully connected network and ReLU activation function\n",
    "# use 30 hidden dimensions\n",
    "# hidden_net = nn.Sequential(...)\n",
    "\n",
    "###########\n",
    "hidden_net = nn.Sequential(\n",
    "    nn.Linear(1, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 30),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "###########\n",
    "\n",
    "mdn = MultivariateGaussianMDN(\n",
    "    features=1,\n",
    "    hidden_net=hidden_net,\n",
    "    num_components=5,\n",
    "    hidden_features=30,\n",
    ")\n",
    "opt = optim.Adam(mdn.parameters(), lr=0.001)\n",
    "for e in range(200):\n",
    "    for x_batch, theta_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "        logits, means, variances = mdn.get_mixture_components(x_batch)\n",
    "        log_probs = mog_log_prob(theta_batch, logits, means, variances)\n",
    "        loss = -log_probs.sum()\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# lets sample from the learned distribution for different x_test\n",
    "samples = []\n",
    "x_test = torch.linspace(-0.1, 1.1, 500).unsqueeze(1)\n",
    "\n",
    "for single_x in x_test:\n",
    "    ### TODO: sample from the learned distribution\n",
    "    # logits, means, variances = ...\n",
    "    # sample = ...\n",
    "\n",
    "    ###########\n",
    "    logits, means, variances = mdn.get_mixture_components(single_x.unsqueeze(1))\n",
    "    sample = mog_sample(logits, means, variances)\n",
    "    ###########\n",
    "\n",
    "    samples.append(sample)\n",
    "samples = torch.cat(samples).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# lets evaluate the posterior at a singe x_demo to get the one dimensionsl MoG density\n",
    "x_demo = torch.as_tensor([[0.6]])\n",
    "# get parameters of the MoG\n",
    "logits, means, variances = mdn.get_mixture_components(x_demo)\n",
    "demo_probs = []\n",
    "for t_demo in torch.linspace(-0.1, 1.0, 100):\n",
    "    # get the log_probs for this all t_demo| x_demo\n",
    "    prob = mog_log_prob(torch.as_tensor([t_demo]), logits, means, variances)\n",
    "    demo_probs.append(prob)\n",
    "demo_probs = torch.stack(demo_probs).detach().exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with mpl.rc_context(fname=\".matplotlibrc\"):\n",
    "    fig = plt.figure(figsize=(4.5, 2.2))\n",
    "    plt.plot(x[:400], theta[:400], \"go\", alpha=0.5, markerfacecolor=\"none\")\n",
    "    plt.plot(x_test, samples.squeeze().detach(), \"ro\", linewidth=3.0)\n",
    "    plt.plot(demo_probs.numpy() * 0.1 + 0.6, np.linspace(-0.1, 1, 100))\n",
    "    plt.plot([0.6] * 100, np.linspace(-0.1, 1, 100), linestyle=\"--\", color=\"k\")\n",
    "    plt.ylim([-0.05, 1.1])\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(r\"$\\theta$\")\n",
    "    # plt.savefig(\"figures/cde_fitted_inv_samples_mdn.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/cde_fitted_inv_samples_mdn.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are the limitations?\n",
    "<br/>\n",
    "- One has to decide on hyperparameters: e.g. how many components (i.e. Gaussians) will I need? <br/>\n",
    "<br/>\n",
    "- Training often does not converge perfectly. <br/>\n",
    "<br/>\n",
    "- In practice, high-dimensional distributions might not be captured perfectly even with a high number of components. <br/>\n",
    "<br/>\n",
    "- Normalizing flows, VAEs, and GANs are more flexible. However, Mixtures of Gaussians often allow for useful computations in closed form and are still sometimes used in practice.<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary: mixture density networks\n",
    "\n",
    "<br/>\n",
    "- Mixture density networks predict parameters of a mixture of Gaussians <br/>\n",
    "<br/>\n",
    "- Because of this, this can capture variability and multi-modality <br/>\n",
    "<br/>\n",
    "- To look at their predictions, one can either draw samples from the predicted mixture of Gaussians or evaluate the probability of the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Acknowledgments\n",
    "<br/>\n",
    "\n",
    "An initial versionn of this notebook was created by Michael Deistler.<br/>\n",
    "Parts of the code from: https://mikedusenberry.com/mixture-density-networks  <br/>\n",
    "Code of MDNs based on Conor Durkan's `lfi` package.  <br/>\n",
    "Bishop, 1994  <br/>\n",
    "[MDN graphic](https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca)  \n",
    "Pedro Gon√ßalves et al. for figure."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "sbi-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
