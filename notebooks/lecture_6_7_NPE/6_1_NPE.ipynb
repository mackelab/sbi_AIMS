{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Posterior Estimation for simulation-based inference\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from sbi.analysis import pairplot\n",
    "from sbi.utils import BoxUniform\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "\n",
    "from ball_throw import throw\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The main idea\n",
    "\n",
    "In Neural Posterior Estimation (NPE) want to use conditional density estimation to learn the posterior $p(\\theta | x)$.\n",
    "\n",
    "As a first step, we have to generate a dataset that follows the joint density $p(\\theta, x)$.\n",
    "\n",
    "We can obtain this by sampling from $p(\\theta)$ (the prior) and then sampling the likelihood $p(x | \\theta)$ (i.e. simulating). \n",
    "\n",
    "The resulting $(\\theta, x)$ pairs follow $p(\\theta, x) = p(\\theta)p(x|\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Posterior estimation: recipe\n",
    "\n",
    "- sample parameters $\\theta$ from prior $p(\\theta)$\n",
    "- run each of these parameters through the (stochastic) simulator to obtain $x \\sim p(x | \\theta)$\n",
    "- train a conditional density estimator on these data to learn $p(\\theta | x)$:\n",
    "\n",
    "<img src=\"figures/npe_illustration.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The simulator\n",
    "In the following example, we will use the physical example of a ball throw. \n",
    "\n",
    "A nice animation and explanation can be found here: http://www.physics.smu.edu/fattarus/ballistic.html\n",
    "\n",
    "We have three free paramters $\\theta = (\\theta_1,\\theta_2,\\theta_3)$ for this simulator:\n",
    "- $\\theta_1$: speed: magnitude of initial speed (m/s).\n",
    "- $\\theta_2$: angle: launch angle with horizontal (degrees)\n",
    "- $\\theta_3$: drag: drag coefficient\n",
    "\n",
    "We assume, that we can only observe a noisy version of the trajectory, because we can only measure the height imprecisely. \n",
    "We simulated this by adding independent Gaussian noise.\n",
    "\n",
    "The implemetation can be found in `simulators/ball_throw.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the simulator\n",
    "velocity = 40\n",
    "angle = 30\n",
    "drag = 0.2\n",
    "sim1 = throw(velocity, angle, drag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and look at the simulation\n",
    "plt.plot(sim1[0], sim1[1])\n",
    "plt.ylim(\n",
    "    0,\n",
    ")\n",
    "plt.title(\"Ball throw\")\n",
    "plt.xlabel(\"distance [m]\")\n",
    "plt.ylabel(\"height [m]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "In principle, we can run NPE on the raw trajectory (more on that later). \n",
    "However, it is often preferable to define summary statistics that are of interest and try reproducing only those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ball_throw import (\n",
    "    get_landing_distance,\n",
    "    get_distance_at_highest_point,\n",
    "    get_highest_point,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_summary_statistics(x):\n",
    "    \"\"\"Calculate summary statistics for results in x\"\"\"\n",
    "\n",
    "    return np.array(\n",
    "        [\n",
    "            get_landing_distance(x),\n",
    "            get_distance_at_highest_point(x),\n",
    "            get_highest_point(x),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbi_throw_with_sumstats(theta, return_raw_sims=False):\n",
    "    \"\"\"Wrapper for throw function to work with SBI.\n",
    "    Arguments:\n",
    "        theta: parameters (batch, 3) for throw\n",
    "    returns:\n",
    "        tensor: summary stats (batch,3)\n",
    "    \"\"\"\n",
    "\n",
    "    sumstats = torch.zeros(theta.shape[0], 3)\n",
    "    sim1 = throw(*theta[0])\n",
    "    sims = np.zeros((theta.shape[0], sim1.shape[-1]))\n",
    "    for i, theta1 in enumerate(theta):\n",
    "        sim1 = throw(*theta1)\n",
    "        sumstats[i] = torch.from_numpy(calculate_summary_statistics(sim1))\n",
    "        sims[i] = sim1[1]\n",
    "\n",
    "    sims[np.isnan(sims)] = 0\n",
    "    if return_raw_sims:\n",
    "        return sumstats, sims\n",
    "    else:\n",
    "        return sumstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check for two different parameter sets\n",
    "theta = torch.tensor([[21, 40, 0.1], [31, 72, 0.01]])\n",
    "sbi_throw_with_sumstats(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question:* Why are the summary statistics not always the same for one parameter $\\theta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The prior\n",
    "We then have to define a prior (a \"first guess of plausible values\"). Here, we pick a uniform distribution within some bounds of reasonable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.utils import BoxUniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question:*\n",
    "\n",
    " What are meaningful boundaries for a Boxuniform distribution for the velocity, angle and drag?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to define a Box Uniform prior, and specify the boundaries here\n",
    "prior_speed = (10, 50)  # m/s\n",
    "prior_angle = (20, 80)  # degree\n",
    "prior_drag = (0.1, 1)  #  drag\n",
    "# define the prior with U(low, high), a box in 3 dimensions\n",
    "prior = BoxUniform(*zip(prior_speed, prior_angle, prior_drag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test by drawing some samples from the prior\n",
    "theta = prior.sample((10,))\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generate simulated data\n",
    "We will run N simulations that will be used to train the conditional density estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N = 100  # number of simulations\n",
    "\n",
    "thetas = prior.sample((N,))\n",
    "\n",
    "xs, sims = sbi_throw_with_sumstats(thetas, return_raw_sims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "# data_dict = dict(thetas=thetas, xs=xs, sims=sims)\n",
    "# with open(\"throw_dataset.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To save the time of simulating: We have already simulated 10_000 traces with different parameters.\n",
    "\n",
    "These were our prior boundaries:\n",
    "\n",
    "prior_speed = (10, 50)  # m/s\n",
    "\n",
    "prior_angle = (20, 80)  # degree\n",
    "\n",
    "prior_drag = (0.1, 1)  #  drag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_simulations = 10_000\n",
    "\n",
    "with open(\"throw_dataset.pickle\", \"rb\") as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "thetas = data_dict[\"thetas\"][:num_simulations]\n",
    "xs = np.array(data_dict[\"xs\"][:num_simulations])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data pre-processing\n",
    "\n",
    "Let's inspect our simulation results. One thing we will realize is that some simulations produce `NaN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The summary statistics of the 1st simulation: \", xs[0])\n",
    "print(\"The summary statistics of the 8th simulation: \", xs[7])\n",
    "print(\"The summary statistics of some simulation: \", xs[1342])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In (Sequential) Neural **Posterior** estimation (SNPE), we can simply exclude those simulations from training for which at least one summary feature is `NaN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "contains_no_nan = np.invert(np.any(np.isnan(xs), axis=1))\n",
    "thetas_train = thetas[contains_no_nan]\n",
    "xs_train = xs[contains_no_nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sometimes, we also have to exclude very large values. \n",
    "These large values could break neural network training.\n",
    "This is not the case here, but will be important for later (e.g. Lotka-Volterra model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "contains_no_inf = np.invert(np.any(xs_train < -1e6, axis=1))\n",
    "thetas_train = thetas_train[contains_no_inf]\n",
    "xs_train = xs_train[contains_no_inf]\n",
    "\n",
    "contains_no_inf = np.invert(np.any(xs_train > 1e6, axis=1))\n",
    "thetas_train = thetas_train[contains_no_inf]\n",
    "xs_train = xs_train[contains_no_inf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We also have to standardize (i.e. z-score) the data $X$ as well as the parameters $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "thetas_torch = torch.as_tensor(thetas_train, dtype=torch.float32)\n",
    "xs_torch = torch.as_tensor(xs_train, dtype=torch.float32)\n",
    "\n",
    "xs_mean = torch.mean(xs_torch, dim=0)\n",
    "xs_std = torch.std(xs_torch, dim=0)\n",
    "xs_zscored = (xs_torch - xs_mean) / xs_std\n",
    "\n",
    "theta_mean = torch.mean(thetas_torch, dim=0)\n",
    "theta_std = torch.std(thetas_torch, dim=0)\n",
    "theta_zscored = (thetas_torch - theta_mean) / theta_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 1: Train neural network to learn $p(\\theta | x)$\n",
    "\n",
    "We now use a Mixture density network to learn the conditional density $p(\\theta | x)$ (=the posterior).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdn import MultivariateGaussianMDN as MultivariateGaussianMDN_diag\n",
    "from mdn import mog_log_prob, mog_sample\n",
    "\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(theta_zscored, xs_zscored)\n",
    "train_loader = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=500,\n",
    ")\n",
    "mdn_diag = MultivariateGaussianMDN_diag(\n",
    "    features=3,  # theta dim\n",
    "    hidden_net=nn.Sequential(\n",
    "        nn.Linear(3, 10),  # input dim: number of summary statistics\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(10, 10),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(10, 20),  # the last hiddden layer should match the hidden_features,\n",
    "    ),\n",
    "    num_components=4,\n",
    "    hidden_features=20,  # what is a meaningful number here?\n",
    ")\n",
    "\n",
    "opt = optim.Adam(mdn_diag.parameters(), lr=0.001)\n",
    "training_loss = []\n",
    "for e in range(100):\n",
    "    for theta_batch, x_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "        weights_of_gaussians, means, variances = mdn_diag.get_mixture_components(\n",
    "            x_batch\n",
    "        )\n",
    "        out = mog_log_prob(theta_batch, weights_of_gaussians, means, variances)\n",
    "        loss = -out.sum()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        training_loss.append(loss.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the loss and see if the network converged\n",
    "plt.plot(np.arange(len(training_loss)) / len(train_loader), training_loss)\n",
    "plt.xlabel(\"training epochs\")\n",
    "plt.ylabel(\"-log p\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Define an observation\n",
    "\n",
    "We will now define an **observation** $x_o$, i.e. the data that we want for which we want to infer the posterior $p(\\theta|x_o)$. In real problems, this will be an experimentally measured trace and we will not know the ground truth parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the simulator with a specific value. this is not known in real problems.\n",
    "velocity = 40\n",
    "angle = 30\n",
    "drag = 0.3\n",
    "# put this into one tensor\n",
    "theta_gt = torch.tensor([velocity, angle, drag])\n",
    "\n",
    "sim_o = throw(velocity, angle, drag)\n",
    "x_o = torch.tensor(calculate_summary_statistics(sim_o))\n",
    "# xo = sbi_throw_with_sumstats(torch.tensor([velocity, angle, drag]).unsqueeze(0))\n",
    "print(\"summary stats for this simulation:\", x_o)\n",
    "\n",
    "plt.plot(sim_o[0], sim_o[1])\n",
    "plt.ylim(\n",
    "    0,\n",
    ")\n",
    "plt.title(\"Ball throw\")\n",
    "plt.xlabel(\"distance [m]\")\n",
    "plt.ylabel(\"height [m]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Because we trained the neural network on z-scored data, we also have to z-score the summary stats of $x_o$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xo_zscored = (x_o - xs_mean) / xs_std\n",
    "xo_torch = torch.as_tensor(xo_zscored, dtype=torch.float32).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Draw samples from the posterior \n",
    "\n",
    "As we are working with summary statistics we have:\n",
    "$p(\\theta | x_o) = p(\\theta | s(sim_o))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 10_000\n",
    "weigths_of_gaussians, means, variances = mdn_diag.get_mixture_components(xo_torch)\n",
    "\n",
    "samples = []\n",
    "for _ in range(n):\n",
    "    samples.append(mog_sample(weigths_of_gaussians, means, variances))\n",
    "\n",
    "samples = torch.cat(samples).detach()\n",
    "samples = samples * theta_std + theta_mean  # de-standardize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "_ = pairplot(\n",
    "    samples,\n",
    "    limits=[prior_speed, prior_angle, prior_drag],\n",
    "    points=[theta_gt],\n",
    "    figsize=(7.5, 7.5),\n",
    "    points_colors=\"r\",\n",
    "    labels=[\"speed [m/s]\", \"angle [deg]\", \"drag\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Questions:* \n",
    "- What can you observe?\n",
    "- What are potential problems?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full MoG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look how this changes for a MoG with full covariance matrices.\n",
    "\n",
    "Here is an implementaion of this:\n",
    "https://github.com/mackelab/pyknos/blob/main/pyknos/mdn/mdn.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyknos.mdn.mdn import MultivariateGaussianMDN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we need to change our code slightly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(theta_zscored, xs_zscored)\n",
    "train_loader = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=500,\n",
    ")\n",
    "\n",
    "mdn = MultivariateGaussianMDN(\n",
    "    features=3,  # theta dim\n",
    "    context_features=3,  # Dimension of inputs.\n",
    "    hidden_features=10,  #  Dimension of final layer of `hidden_net`.\n",
    "    hidden_net=nn.Sequential(\n",
    "        nn.Linear(3, 10),  # input dim\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(10, 10),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(10, 10),\n",
    "    ),\n",
    "    num_components=3,\n",
    ")\n",
    "\n",
    "opt = optim.Adam(mdn.parameters(), lr=0.001)\n",
    "training_loss = []\n",
    "for e in range(50):\n",
    "    for theta_batch, x_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        out = mdn.log_prob(theta_batch, x_batch)\n",
    "        # weights_of_gaussians, means, variances = mdn.get_mixture_components(x_batch)\n",
    "        # out = mog_log_prob(theta_batch, weights_of_gaussians, means, variances)\n",
    "        loss = -out.sum()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        training_loss.append(loss.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the loss and see if the network converged\n",
    "plt.plot(np.arange(len(training_loss)) / len(train_loader), training_loss)\n",
    "plt.xlabel(\"training epochs\")\n",
    "plt.ylabel(\"-log p\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1_000\n",
    "\n",
    "samples_posterior = mdn.sample(n, xo_torch).detach().squeeze()\n",
    "samples_posterior = (\n",
    "    samples_posterior * theta_std + theta_mean\n",
    ")  # de-standardize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pairplot(\n",
    "    samples_posterior,\n",
    "    limits=[prior_speed, prior_angle, prior_drag],\n",
    "    points=[theta_gt],\n",
    "    figsize=(7.5, 7.5),\n",
    "    points_colors=\"r\",\n",
    "    labels=[\"speed [m/s]\", \"angle [deg]\", \"drag\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to evaluate that this is correct?\n",
    "\n",
    "More on this later! But a quick check are **Posterior predictive checks**. We draw parameters from the posterior, simulate them, and inspect whether the resulting traces match $x_o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# get the ground truth simulation\n",
    "gt_simulation = throw(*theta_gt)\n",
    "\n",
    "# get the posterior simulation\n",
    "posterior_simulation = [throw(*samples_posterior[i]) for i in range(20)]\n",
    "\n",
    "# get the prior simulation for comparison\n",
    "prior_simulation = [throw(*thetas[i]) for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = gt_simulation[0]\n",
    "with mpl.rc_context(fname=\".matplotlibrc\"):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "    ax[0].plot(\n",
    "        d,\n",
    "        prior_simulation[0][1],\n",
    "        \"black\",\n",
    "        label=\"prior predictive\",\n",
    "        alpha=0.5,\n",
    "        lw=0.5,\n",
    "    )\n",
    "    for i in range(1, 20):\n",
    "        ax[0].plot(d, prior_simulation[i][1], \"black\", alpha=0.5, lw=0.5)\n",
    "\n",
    "    ax[0].plot(d, gt_simulation[1], color=\"r\", label=\"ground truth\")\n",
    "\n",
    "    ax[0].legend()\n",
    "    ax[0].set_ylim(0, 30)\n",
    "    ax[0].set_xlim(0, 150)\n",
    "    ax[0].set_title(\"Prior predictive\")\n",
    "    ax[0].set_xlabel(\"distance [m]\")\n",
    "    ax[0].set_ylabel(\"height [m]\")\n",
    "\n",
    "    ax[1].plot(\n",
    "        d,\n",
    "        posterior_simulation[0][1],\n",
    "        \"b-\",\n",
    "        label=\"prior predictive\",\n",
    "        alpha=0.5,\n",
    "        lw=0.5,\n",
    "    )\n",
    "    for i in range(20):\n",
    "        ax[1].plot(d, posterior_simulation[i][1], \"b-\", alpha=0.5, lw=0.5)\n",
    "\n",
    "    ax[1].plot(d, gt_simulation[1], color=\"r\", label=\"ground truth\")\n",
    "\n",
    "    ax[1].legend()\n",
    "    ax[1].set_ylim(0, 30)\n",
    "    ax[1].set_xlim(0, 150)\n",
    "    ax[1].set_title(\"Posterior predictive\")\n",
    "    ax[1].set_xlabel(\"distance [m]\")\n",
    "\n",
    "    # plt.savefig(\"figures/post_predictives.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 2: Try different number of training samples\n",
    "\n",
    "- first, familiarize yourself with the code above and make sure you understand what's going on.\n",
    "- Then, go back to the cell in which we loaded the presimulated data:\n",
    "```python\n",
    "num_simulations = 10_000\n",
    "\n",
    "with open(\"data/throw_dataset.pickle\", \"rb\") as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "thetas = data_dict[\"thetas\"][:num_simulations]\n",
    "xs = np.array(data_dict[\"xs\"][:num_simulations])\n",
    "```\n",
    "- try training the neural network and evaluating the posterior with fewer simulations. What do you observe as you go to around $500$ (or even fewer) simulations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Congrats, you understood the basics of NPE!\n",
    "\n",
    "Let's move on to some cool features..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Amortization\n",
    "\n",
    "One of the cool features of NPE is that the posterior is **amortized**. This means that, after the simulations are done and the network is trained, one can quickly obtain the posterior for any observation $x_o$ (a single forward pass through the neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Exercise 3: Test amortization for a few different $x_o$\n",
    "\n",
    "Use the code cells below to test amortization. In other words: change the parameters used to generate observed data (`gt2 = ...`) and inspect whether the posterior samples match the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "theta_gt2 = torch.tensor([40.0, 70.0, 0.2])  # [velocity, angle, drag])\n",
    "# remember ther prior bounds:\n",
    "# prior_speed = (10, 50)  # m/s\n",
    "# prior_angle = (20, 80)  # degree\n",
    "# prior_drag = (0.1, 1)  # (0.05, 0.3)  # drag\n",
    "\n",
    "sim_o2 = throw(*theta_gt2)\n",
    "xo2 = torch.tensor(calculate_summary_statistics(sim_o2), dtype=torch.float)\n",
    "# xo = sbi_throw_with_sumstats(torch.tensor([velocity, angle, drag]).unsqueeze(0))\n",
    "print(\"summary stats for this simulation:\", xo2)\n",
    "# z-score the summary stats\n",
    "xo_torch2 = (xo2 - xs_mean) / xs_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question:* What happens if you put in a `theta_gt2` which is outside of the prior bounds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "samples = mdn.sample(n, xo_torch2.unsqueeze(0)).detach().squeeze()\n",
    "samples = samples * theta_std + theta_mean  # de-standardize the parameters\n",
    "_ = pairplot(\n",
    "    samples,\n",
    "    limits=[prior_speed, prior_angle, prior_drag],\n",
    "    points=[theta_gt2],\n",
    "    figsize=(7.5, 7.5),\n",
    "    points_colors=\"r\",\n",
    "    labels=[\"speed [m/s]\", \"angle [deg]\", \"drag\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_simulation2 = throw(*theta_gt2)\n",
    "\n",
    "posterior_simulation2 = [throw(*samples[i]) for i in range(20)]\n",
    "\n",
    "\n",
    "prior_simulation = [throw(*thetas[i]) for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = gt_simulation[0]\n",
    "with mpl.rc_context(fname=\".matplotlibrc\"):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "    # Prior\n",
    "    ax[0].plot(\n",
    "        d,\n",
    "        prior_simulation[0][1],\n",
    "        \"black\",\n",
    "        label=\"prior predictive\",\n",
    "        alpha=0.5,\n",
    "        lw=0.5,\n",
    "    )\n",
    "    for i in range(1, 20):\n",
    "        ax[0].plot(d, prior_simulation[i][1], \"black\", alpha=0.5, lw=0.5)\n",
    "\n",
    "    ax[0].plot(d, gt_simulation2[1], color=\"r\", label=\"ground truth\")\n",
    "\n",
    "    ax[0].legend()\n",
    "    ax[0].set_ylim(0, 60)\n",
    "    ax[0].set_xlim(0, 150)\n",
    "    ax[0].set_title(\"Prior predictive\")\n",
    "    ax[0].set_xlabel(\"distance [m]\")\n",
    "    ax[0].set_ylabel(\"height [m]\")\n",
    "\n",
    "    # Posterior\n",
    "    ax[1].plot(\n",
    "        d,\n",
    "        posterior_simulation2[0][1],\n",
    "        \"b-\",\n",
    "        label=\"prior predictive\",\n",
    "        alpha=0.5,\n",
    "        lw=0.5,\n",
    "    )\n",
    "    for i in range(1, 20):\n",
    "        ax[1].plot(d, posterior_simulation2[i][1], \"b-\", alpha=0.5, lw=0.5)\n",
    "\n",
    "    ax[1].plot(d, gt_simulation2[1], color=\"r\", label=\"ground truth\")\n",
    "\n",
    "    ax[1].legend()\n",
    "    ax[1].set_ylim(0, 60)\n",
    "    ax[1].set_xlim(0, 150)\n",
    "    ax[1].set_title(\"Posterior predictive\")\n",
    "    ax[1].set_xlabel(\"distance [m]\")\n",
    "\n",
    "    # plt.savefig(\"figures/post_predictives2.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Embedding network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So far, we used summary statistics of the raw trace (i.e. landing distance, highest point etc.).\n",
    "\n",
    "In some cases, you might not want to (or can not) define summary statistics. What to do then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can learn summary statistics! One can pass the simulated data $x$ through **any** neural network before regressing on the Mixture Parameters (e.g. CNN, LSTM, GNN,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"figures/cnn.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "The network will automatically extract relevant features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "num_simulations = 10_000\n",
    "\n",
    "with open(\"throw_dataset.pickle\", \"rb\") as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "thetas = data_dict[\"thetas\"][:num_simulations]\n",
    "xs = np.array(\n",
    "    data_dict[\"sims\"][:num_simulations]\n",
    ")  # <-- we now load the raw simulations instead\n",
    "\n",
    "# Filter nans\n",
    "contains_no_nan = np.invert(np.any(np.isnan(xs), axis=1))\n",
    "thetas_train = thetas[contains_no_nan]\n",
    "xs_train = xs[contains_no_nan]\n",
    "\n",
    "# z-score data\n",
    "thetas_torch = torch.as_tensor(thetas_train, dtype=torch.float32)\n",
    "xs_torch = torch.as_tensor(xs_train, dtype=torch.float32)\n",
    "\n",
    "xs_mean = torch.mean(xs_torch, dim=0)\n",
    "xs_std = torch.std(xs_torch, dim=0)\n",
    "xs_zscored = (xs_torch - xs_mean) / xs_std\n",
    "\n",
    "theta_mean = torch.mean(thetas_torch, dim=0)\n",
    "theta_std = torch.std(thetas_torch, dim=0)\n",
    "theta_zscored = (thetas_torch - theta_mean) / theta_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the shape of our observations\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define the embedding network as a fully connected network\n",
    "hidden_net = nn.Sequential(\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "# define the embedding network\n",
    "hidden_net = nn.Sequential(\n",
    "    nn.Linear(151, 50),  # input dim,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 20),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(theta_zscored, xs_zscored)\n",
    "train_loader = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=500,\n",
    ")\n",
    "\n",
    "\n",
    "mdn = MultivariateGaussianMDN(\n",
    "    features=3,  # theta dim\n",
    "    context_features=151,  # Dimension of inputs. this is our raw data dimension now.\n",
    "    hidden_features=20,  #  Dimension of final layer of `hidden_net`.\n",
    "    hidden_net=hidden_net,  # <--here goes the CNN, LSTM, GNN,..\n",
    "    num_components=3,\n",
    ")\n",
    "\n",
    "\n",
    "opt = optim.Adam(mdn.parameters(), lr=0.001)\n",
    "training_loss = []\n",
    "for e in range(100):\n",
    "    for theta_batch, x_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        out = mdn.log_prob(theta_batch, x_batch)\n",
    "        # weights_of_gaussians, means, variances = mdn.get_mixture_components(x_batch)\n",
    "        # out = mog_log_prob(theta_batch, weights_of_gaussians, means, variances)\n",
    "        loss = -out.sum()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        training_loss.append(loss.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the loss and see if the network converged\n",
    "plt.plot(np.arange(len(training_loss)) / len(train_loader), training_loss)\n",
    "plt.xlabel(\"training epochs\")\n",
    "plt.ylabel(\"-log p\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_gt = torch.tensor([40.0, 50.0, 0.6])  # [velocity, angle, drag])\n",
    "# remember ther prior bounds:\n",
    "# prior_speed = (10, 50)  # m/s\n",
    "# prior_angle = (20, 80)  # degree\n",
    "# prior_drag = (0.1, 1)  # (0.05, 0.3)  # drag\n",
    "\n",
    "\n",
    "sumstatsxo, xo = sbi_throw_with_sumstats(theta_gt.unsqueeze(0), return_raw_sims=True)\n",
    "# z-score the simulation\n",
    "xo = torch.tensor(xo, dtype=torch.float)\n",
    "xo_torch = (xo - xs_mean) / xs_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "\n",
    "samples = mdn.sample(n, xo_torch).detach().squeeze()\n",
    "samples = samples * theta_std + theta_mean  # de-standardize the parameters\n",
    "_ = pairplot(\n",
    "    samples,\n",
    "    limits=[prior_speed, prior_angle, prior_drag],\n",
    "    points=[theta_gt],\n",
    "    figsize=(7.5, 7.5),\n",
    "    points_colors=\"r\",\n",
    "    labels=[\"speed [m/s]\", \"angle [deg]\", \"drag\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question:*\n",
    "\n",
    "The posterior marginals seem to be tighter, can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Should I use an embedding net?\n",
    "\n",
    "Advantages:\n",
    "- No need for hand-selected features\n",
    "- possible insights into which features are learned by the CNN, LSTM,...\n",
    "\n",
    "Disadvantages:\n",
    "- Probably more training data needed to learn useful features\n",
    "- The embedding net can learn suspicious simulation effects (e.g. initial value etc.) which may not be interesting for the real data, but highly informative for the posterior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "Neural Posterior Estimation (NPE) works as follows:\n",
    "- sample the prior: $\\theta \\sim p(\\theta)$\n",
    "- run the simulator for each parameter: $x \\sim p(x | \\theta)$\n",
    "- train a conditional density estimator $q(\\theta | x)$.\n",
    "- after training, plug the observed data $x_o$ into the network to obtain the posterior.\n",
    "\n",
    "Benefits:\n",
    "- after training, the posterior is **amortized**, i.e. it can rapidly be evaluated for new data (no new simulations or retraining)\n",
    "- NPE can automatically learn summary statistics with the embedding net\n",
    "\n",
    "In the last week we will see Sequential Neural Posterior Estimation (SNPE), which performs inference over multiple rounds.\n",
    "- This can enhance the simulation efficiency\n",
    "- But it requires changes to the loss function. The SNPE algorithms differ in how they deal with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank you for your attention!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "sbi-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
