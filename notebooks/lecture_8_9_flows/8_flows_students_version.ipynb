{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Flows\n",
    "\n",
    "Normalizing flows are a class of models that can be used to represent complex densities. They are based on the following idea: given a simple density $p_0(z)$, we can transform it into a more complex density $p_k(z)$ by applying a sequence of invertible transformations $f_k = f \\circ f_{k-1} \\circ \\dots \\circ f_1$:\n",
    "The change of variables formula gives us: \n",
    "$$ p_k(z) = p_0(f_k^{-1}(z)) \\left| \\det \\frac{\\partial f_k^{-1}(z)}{\\partial z} \\right| \\qquad \\qquad (1)$$ \n",
    "where $\\left| \\det \\frac{\\partial f_k(z)}{\\partial z} \\right|$ is the Jacobian determinant of $f_k$.\n",
    "\n",
    "Unfortunately, naively applying this idea is not practical as the Jacobian determinant is very expensive to compute. Yet, as it turns out we can especially design the transformations $f_k$ such that the Jacobian determinant is easy to compute.\n",
    "\n",
    "### Affine transformations\n",
    "\n",
    "The simplest transformation is an affine transformation:\n",
    "$$ f(z) = \\mu + \\sigma \\odot z $$\n",
    "where $\\sigma \\neq 0$ are learnable parameters and $\\odot$ is the element-wise product. The Jacobian determinant of this transformation is:\n",
    "\n",
    "$$ \\left| \\det \\frac{\\partial f(z)}{\\partial z} \\right| = \\left| \\det \\sigma \\right| = \\prod_{i=1}^D \\sigma_i $$\n",
    "where $D$ is the dimensionality of $z$.\n",
    "\n",
    "And the inverse transformation is:\n",
    "$$ f^{-1}(z) = \\frac{z - \\mu}{\\sigma} $$\n",
    "\n",
    "So what is the determinant $\\frac{\\partial f^{-1}(z)}{\\partial z}$ of the inverse transformation  (the quantity we actually need in formula (1))?\n",
    "\n",
    "So let's implement this transforation and its inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 1*:\n",
    "\n",
    "Complete the following `affine_bijectory` and its inverse function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_bijector(params, x):\n",
    "    \"\"\"\n",
    "    transforms x with affine transformation\n",
    "\n",
    "    args:\n",
    "        params: shape (batch, dim, 2)\n",
    "        x: shape (batch, dim)\n",
    "    return:\n",
    "        y: shape (batch, dim)\n",
    "    \"\"\"\n",
    "    shift = # TODO: which dimensions of params correspond to shift?\n",
    "    scale = # TODO: which dimensions of params correspond to scale?\n",
    "    scale = torch.exp(scale)  # make the scale positive\n",
    "    return # TODO: implement affine transformation of x\n",
    "\n",
    "\n",
    "def affine_bijector_inv_and_log_det(params, y):\n",
    "    \"\"\"\n",
    "    transforms y with inverse affine transformation\n",
    "\n",
    "    args:\n",
    "        params: shape (batch, dim, 2)\n",
    "        y: shape (batch, dim)\n",
    "    return: x, log_det\n",
    "    \"\"\"\n",
    "    shift = # TODO: which dimensions of params correspond to shift?\n",
    "    scale = # TODO: which dimensions of params correspond to scale?\n",
    "    scale = torch.exp(scale) # make the scale positive\n",
    "\n",
    "    x = # TODO: implement inverse affine transformation of y\n",
    "    log_det = # TODO: what is the log determinant of the Jacobian of the inverse affine transformation?\n",
    "    return x, log_det"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following three cells you can check if your implementation is correct. \n",
    "\n",
    "Be aware that we implemented the `params` argument in such a way that each `x` has its own parameters (the batch dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for some parameters and some data x_test\n",
    "\n",
    "# a batch with 3 elements\n",
    "x_test = torch.tensor([[1.0, 2.0], [3.0, 4.0], [3.0, 7.0]])\n",
    "\n",
    "# here we need to repeat the params as we have 3 data points x:\n",
    "# so we will have dimension of (batch, 2, 2) with batch = 3\n",
    "params = torch.tensor([[1.0, 3.0], [2.0, 1.0]]).repeat(3, 1, 1)\n",
    "\n",
    "# test affine transformation\n",
    "ys = affine_bijector(params, x_test)\n",
    "\n",
    "# test inverse affine transformation and log determinant\n",
    "xs_rec, log_det = affine_bijector_inv_and_log_det(params, ys)\n",
    "\n",
    "print(\"Recovered tensors are the same:\", torch.isclose(xs_rec, x_test).all())\n",
    "print(\"log_det:\", log_det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the `log_det` correct? How can you check this on the example above? \n",
    "\n",
    "*Hint:* \n",
    "To transform the scales to positive numbers, we take the $\\exp$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on one dimensional distribution\n",
    "To have a closer look at the `log_det` we will test the transformation on a one dimensional distribution.\n",
    "\n",
    "For this we will check if the transformed samples follow the transformed pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for random one dimensional data\n",
    "\n",
    "# define base distribution p0\n",
    "p0 = torch.distributions.Normal(torch.tensor(0.0), torch.tensor([1.0]))\n",
    "\n",
    "# define number of samples\n",
    "n = 2_000\n",
    "\n",
    "# if we want to have a fixed transformation for the whole batch,\n",
    "# we need to repeat the parameters n times\n",
    "params = torch.tensor([[2.0, 1.0]]).repeat(\n",
    "    n, 1, 1\n",
    ")  # shift and scale. Remember: scale is in log space!\n",
    "\n",
    "# sample from base distribution\n",
    "xs = p0.sample((n,))\n",
    "# calculate log probability of samples\n",
    "log_prob_xs = p0.log_prob(xs)\n",
    "\n",
    "# transform samples\n",
    "ys = affine_bijector(params, xs)\n",
    "\n",
    "# inverse transform and compute log determinant\n",
    "xs_rec, log_det = affine_bijector_inv_and_log_det(params, ys)\n",
    "\n",
    "# calculate log probability of transformed samples\n",
    "log_prob_ys = p0.log_prob(xs_rec).squeeze() + log_det.squeeze()\n",
    "# Can you explain why we need to add the log determinant to the log_prob?\n",
    "\n",
    "\n",
    "# We can now plot samples from the transformed distribution as well as\n",
    "# the transformed density function to check if the log_det is correct\n",
    "s1 = plt.plot(\n",
    "    xs.squeeze(),\n",
    "    torch.exp(log_prob_xs.squeeze()).numpy(),\n",
    "    \"o\",\n",
    "    label=\"p0: Base distribution\",\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "s2 = plt.plot(\n",
    "    ys.squeeze(),\n",
    "    torch.exp(log_prob_ys.squeeze()).numpy(),\n",
    "    \"o\",\n",
    "    label=\"p1: Transformed distribution\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "# plot histograms of the samples\n",
    "_ = plt.hist(xs.numpy(), bins=20, density=True, alpha=0.5, color=\"red\")\n",
    "_ = plt.hist(ys.numpy(), bins=50, density=True, alpha=0.5, color=\"blue\")\n",
    "plt.legend()\n",
    "\n",
    "print(\"Recovered tensors are the same:\", torch.isclose(xs_rec, xs, atol=1e-3).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look how a two dimensional transformed distribution might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for two dimensional data\n",
    "\n",
    "# define base distribution p0 as a multivariate normal distribution\n",
    "p0 = torch.distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "\n",
    "# sample from the base distribution\n",
    "n = 2_000\n",
    "xs = p0.sample((n,))\n",
    "\n",
    "# define parameters for affine transformation (and again repeat them for the whole batch)\n",
    "params = torch.tensor([[3.0, 0.1], [-4.0, 2.0]]).repeat(n, 1, 1)\n",
    "\n",
    "# transform samples and calculate log probability of samples\n",
    "ys = affine_bijector(params, xs)\n",
    "log_prob_xs = p0.log_prob(xs)\n",
    "\n",
    "# inverse transform and log probs\n",
    "xs_rec, log_det = affine_bijector_inv_and_log_det(params, ys)\n",
    "\n",
    "log_prob_ys = p0.log_prob(xs_rec) + log_det\n",
    "\n",
    "# Plot a two dimensional kernel density estimation (kde)\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=xs[:, 0].numpy(),\n",
    "    y=xs[:, 1].numpy(),\n",
    "    fill=False,\n",
    "    cmap=\"Reds\",\n",
    "    color=\"red\",\n",
    "    label=\"p0: Base distribution\",\n",
    "    vmin=0,\n",
    ")\n",
    "sns.kdeplot(\n",
    "    x=ys[:, 0].numpy(),\n",
    "    y=ys[:, 1].numpy(),\n",
    "    fill=False,\n",
    "    cmap=\"Blues\",\n",
    "    label=\"p1: Transformed distribution\",\n",
    ")\n",
    "\n",
    "# lets add colored patches to the legend\n",
    "handles = [\n",
    "    mpatches.Patch(facecolor=plt.cm.Reds(100), label=\"p0: Base distribution\"),\n",
    "    mpatches.Patch(facecolor=plt.cm.Blues(100), label=\"p1: Transformed distribution\"),\n",
    "]\n",
    "plt.legend(handles=handles)\n",
    "\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "\n",
    "\n",
    "print(\"Recovered tensors are the same:\", torch.isclose(xs_rec, xs, atol=1e-5).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We now already can tranform base distributions with an affine transformation.\n",
    "\n",
    "The downside is: the transformed distribution is still a Gaussian....\n",
    "\n",
    "So what to do...????\n",
    "\n",
    "Well, we could transform some dimensions with a function that depends on the other dimensions, so called "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coupling layers\n",
    "\n",
    "Problematically affine transformations alone won't help us to represent complex densities. An affine transformation of a Gaussian is still a Gaussian. To overcome this problem, we can use **coupling layers**. The idea is to split the input $z$ into two parts $z_1$ and $z_2$ and only transform $z_2$:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "z_1, z_2 &= \\text{split}(z)\\\\\n",
    "z_1' &= z_1 \\\\\n",
    "z_2' &= z_2 \\odot \\exp(g_{\\phi_2}(z_1)) + g_{\\phi_1}(z_1)  \n",
    "\\end{align*} $$\n",
    "\n",
    "The Jacobian determinant of this transformation still has a simple form:\n",
    "$$ \\left| \\det \\frac{\\partial f(z)}{\\partial z} \\right| = \\exp \\left( \\sum_{i=1}^D g_{\\phi_2}^i(z_1) \\right) $$\n",
    "which you can easily verify yourself.\n",
    "\n",
    "Further we can easily compute the inverse transformation:\n",
    "$$ \\begin{align*}\n",
    "z_1, z_2 &= \\text{split}(z)\\\\\n",
    "z_1' &= z_1 \\\\\n",
    "z_2' &= (z_2 - g_{\\phi_1}(z_1)) \\odot \\exp(-g_{\\phi_2}(z_1))\n",
    "\\end{align*} $$\n",
    "\n",
    "Take a close look: These are the same formulas as above, but we replaced the constant shift and scale by the functions \n",
    " $g_{\\phi_1}$  and $\\exp(g_{\\phi_2})$.\n",
    "\n",
    "We can now parameterize $g_{\\phi_1}$ and $g_{\\phi_2}$ using neural networks. This allows us to learn complex transformations $f$ that can be used to represent complex densities.\n",
    "\n",
    "Notably with a single coupling layer, we do not change the distribution of $z_1$ and only transform $z_2$. Therefor we will stack several layers later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important:\n",
    "To use coupling layers for **conditional** density estimation, the functions $g_i$ will additionally take a context variable as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task*:\n",
    "\n",
    "Complete the code below to implement one single coupling layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CouplingNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        context_dim,\n",
    "        bijector_dim,\n",
    "        bijector_fn,\n",
    "        bijector_inverse_and_log_det_fn,\n",
    "        hidden_dims=[50, 50],\n",
    "        act=nn.ReLU(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        implements a neural network which parameterizes the bijector.\n",
    "        args:\n",
    "            input_dim: dimension of the input\n",
    "            context_dim: dimension of the context\n",
    "            bijector_dim: needed dimensions for the bijector\n",
    "            bijector_fn: function which implements the forward pass of the bijector\n",
    "            bijector_inverse_and_log_det_fn: function which implements the inverse pass of the bijector\n",
    "            hidden_dims: list of hidden dimensions\n",
    "            act: activation function\n",
    "        \"\"\"\n",
    "        super(CouplingNet, self).__init__()\n",
    "\n",
    "        self.bijector_fn = bijector_fn\n",
    "        self.bijector_inverse_and_log_det_fn = bijector_inverse_and_log_det_fn\n",
    "        self.bijector_dim = bijector_dim\n",
    "\n",
    "        # Let's construct the neural network which will be used to parameterize the bijector\n",
    "        self.n_change = input_dim // 2  # number of dimensions to modify\n",
    "        self.n_unchange = (\n",
    "            input_dim - self.n_change\n",
    "        )  # number of dimensions to keep unchanged\n",
    "        random_dims = torch.randperm(input_dim)  # random permutation of the dimensions\n",
    "        self.changedim = random_dims[\n",
    "            : self.n_change\n",
    "        ]  # choose the the dimensions to change\n",
    "        self.unchangedim = random_dims[\n",
    "            self.n_change :\n",
    "        ]  # choose the the dimensions to keep unchanged\n",
    "        output_dim = (\n",
    "            # TODO: what should the output dimension of the network be?\n",
    "        )  \n",
    "        block = [nn.Linear(#TODO: what is th input of the first linear layer?\n",
    "            , hidden_dims[0]), act]\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            block += [nn.Linear(hidden_dims[i - 1], hidden_dims[i]), act]\n",
    "        block += [nn.Linear(hidden_dims[-1], output_dim)]\n",
    "        self.net = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        \"\"\"This will implement the forward pass of the coupling layer.\"\"\"\n",
    "        # split the input into the dimensions to change and the dimensions to keep unchanged\n",
    "        x1 = x[:, self.unchangedim]\n",
    "        x2 = x[:, self.changedim]\n",
    "        # calculate the parameters of the bijector,\n",
    "        # dependent on the dimensions to keep unchanged and the context\n",
    "        bijector_params = self.net(torch.cat([x1, context], dim=-1)).reshape(\n",
    "            # TODO: we have to reshape the output of the network to the correct shape. \n",
    "            # What should the shape be?\n",
    "        )\n",
    "        # apply the bijector\n",
    "        y2 = self.bijector_fn(bijector_params, x2)\n",
    "        # replace the dimensions to change with the transformed dimensions\n",
    "        x[:, self.changedim] = y2\n",
    "        return x\n",
    "\n",
    "    def inverse(self, y, context):\n",
    "        \"\"\"This will implement the inverse pass of the coupling layer.\"\"\"\n",
    "        y1 = # TODO\n",
    "        y2 = # TODO\n",
    "        bijector_params = self.net(torch.cat([y1, context], dim=-1)).reshape(\n",
    "            # TODO\n",
    "        )\n",
    "        x2, log_det = self.bijector_inverse_and_log_det_fn(bijector_params, y2)\n",
    "        y[:, self.changedim] = #TODO\n",
    "\n",
    "        return y, log_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the CouplingNet for some random data\n",
    "input_dim = 10\n",
    "context_dim = 2\n",
    "n = 10  # number of samples\n",
    "# create random data\n",
    "x = torch.randn(n, input_dim)\n",
    "# create some random context (same number of samples as x)\n",
    "context = torch.randn(n, context_dim)\n",
    "\n",
    "# initialise a CouplingNet\n",
    "coupling_net = CouplingNet(\n",
    "    input_dim, context_dim, 2, affine_bijector, affine_bijector_inv_and_log_det\n",
    ")  # bijector dim is 2 (shift and scale)\n",
    "\n",
    "# apply the forward pass\n",
    "y = coupling_net(x, context)\n",
    "\n",
    "# apply the inverse pass\n",
    "x_rec, log_det = coupling_net.inverse(y, context)\n",
    "\n",
    "# check if x and x_rec are the same\n",
    "print(\"The inverse is working for all x: \", torch.allclose(x, x_rec, atol=1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing flows with coupling layers\n",
    "\n",
    "We can now use coupling layers to construct a normalizing flow. We start with a simple density $p_0(z)$ and then apply a sequence of coupling layers to obtain a more complex density $p_k(z)$.\n",
    "To address the problem that only $z_2$ is transformed, we can just choose different dimensions to transfor in each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: comment the code below to explain what is happening\n",
    "\n",
    "\n",
    "class NormalizingFlow(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        context_dim,\n",
    "        bijector_dim,\n",
    "        bijector_fn=affine_bijector,\n",
    "        bijector_inverse_and_log_det_fn=affine_bijector_inv_and_log_det,\n",
    "        num_layers=5,\n",
    "        hidden_dims=[50, 50],\n",
    "        act=nn.ReLU(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implements a normalizing flow with coupling layers.\n",
    "        args:\n",
    "            input_dim: dimension of the input\n",
    "            context_dim: dimension of the context\n",
    "            bijector_dim: needed dimensions for the bijector\n",
    "            bijector_fn: function which implements the forward pass of the bijector\n",
    "            bijector_inverse_and_log_det_fn: function which implements the inverse pass of the bijector\n",
    "            num_layers: number of coupling layers\n",
    "            hidden_dims: list of hidden dimensions per coupling layer\n",
    "            act: activation function\n",
    "        \"\"\"\n",
    "        super(NormalizingFlow, self).__init__()\n",
    "        self.bijectors = nn.ModuleList(\n",
    "            [\n",
    "                CouplingNet(\n",
    "                    input_dim,\n",
    "                    context_dim,\n",
    "                    bijector_dim,\n",
    "                    bijector_fn,\n",
    "                    bijector_inverse_and_log_det_fn,\n",
    "                    hidden_dims,\n",
    "                    act,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.base_dist = torch.distributions.Independent(\n",
    "            torch.distributions.Normal(torch.zeros(input_dim), torch.ones(input_dim)), 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        y = x\n",
    "        for bijector in self.bijectors:\n",
    "            y = bijector(y, context)\n",
    "        return y\n",
    "\n",
    "    def log_prob(self, y, context):\n",
    "        x, log_det = self.inverse(y, context)\n",
    "        return self.base_dist.log_prob(x) + log_det\n",
    "\n",
    "    def sample(self, num_samples, context):\n",
    "        \"\"\"samples from flow distribution for one context\"\"\"\n",
    "        # sample from base distribution\n",
    "        y = self.base_dist.sample((num_samples,))\n",
    "        # repeat the context s.t. it also has a batch dimension\n",
    "        context = torch.broadcast_to(context, y.shape[:-1] + context.shape[-1:])\n",
    "        for bijector in self.bijectors:\n",
    "            y = bijector(y, context)\n",
    "        return y\n",
    "\n",
    "    def inverse(self, y, context):\n",
    "        x = y\n",
    "        log_det = 0\n",
    "        for bijector in reversed(\n",
    "            self.bijectors\n",
    "        ):  # we need to reverse the order of the bijectors!!!\n",
    "            x, log_det_ = bijector.inverse(x, context)\n",
    "            log_det += log_det_\n",
    "        return x, log_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 7\n",
    "context_dim = 3\n",
    "bijector_dim = 2\n",
    "net = NormalizingFlow(\n",
    "    input_dim, context_dim, bijector_dim, affine_bijector, num_layers=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, input_dim)\n",
    "context = torch.randn(10, context_dim)\n",
    "y = net(x, context)\n",
    "x_rec, log_det = net.inverse(y, context)\n",
    "\n",
    "# check if x and x_rec are the same\n",
    "print(\"The inverse is working for all x: \", torch.allclose(x, x_rec, atol=1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Flows on Two Moons dataset\n",
    "\n",
    "We can now use normalizing flows to construct expressive conditional density estimators $q_\\phi(\\theta|x)$. Let's look at the same example of the two moons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moons(n, noise=0.1, mode=\"fixed\", shift=torch.tensor([[0.0, 0.0], [0.0, 0.0]])):\n",
    "    \"\"\"Returns moons dataset with two moons and labels.\n",
    "    Args:\n",
    "        n: number of samples\n",
    "        noise: noise level\n",
    "        shift: shift of the moons. (2,2) or \"random\"\n",
    "    Returns:\n",
    "        X: shape (n, 2)\n",
    "        ids: shape (n, 1)\n",
    "    \"\"\"\n",
    "    X, ids = make_moons(n_samples=n, noise=noise)\n",
    "    X = torch.tensor(X, dtype=torch.float)\n",
    "    ids = torch.tensor(ids, dtype=torch.float)\n",
    "\n",
    "    # shift the moons\n",
    "    if mode == \"fixed\":\n",
    "        X[ids == 0] += shift[0]\n",
    "        X[ids == 1] += shift[1]\n",
    "    elif mode == \"random\":\n",
    "        shift = (torch.rand(n, 2) - 0.5) * 10\n",
    "        X += shift\n",
    "\n",
    "    return X, ids.unsqueeze(-1), shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = torch.tensor([[-2.0, 1.0], [0.0, 0.0]])\n",
    "\n",
    "X, context, shift = get_moons(1_000, shift=shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "plt.plot(\n",
    "    X[context.squeeze() == 0, 0],\n",
    "    X[context.squeeze() == 0, 1],\n",
    "    \".\",\n",
    "    alpha=0.5,\n",
    "    label=\"context 0\",\n",
    ")\n",
    "plt.plot(\n",
    "    X[context.squeeze() == 1, 0],\n",
    "    X[context.squeeze() == 1, 1],\n",
    "    \".\",\n",
    "    alpha=0.5,\n",
    "    label=\"context 1\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Two Moons Dataset\")\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-3, 3)\n",
    "\n",
    "# plt.plot(shift[:, 0], shift[:, 1], \"x\", color=\"red\", label=\"shifts\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cretae a dataset\n",
    "dataset = data.TensorDataset(X, context)\n",
    "# define a dataloader\n",
    "train_loader = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard optimizer used across a lot of neural network tasks\n",
    "\n",
    "input_dim = 2\n",
    "context_dim = 1\n",
    "bijector_dim = 2\n",
    "\n",
    "flow = NormalizingFlow(\n",
    "    input_dim, context_dim, bijector_dim, affine_bijector, num_layers=5\n",
    ")\n",
    "opt = optim.Adam(flow.parameters(), lr=0.001)\n",
    "\n",
    "# training loop\n",
    "for e in range(100):\n",
    "    for x_batch, context_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "        log_probs = flow.log_prob(x_batch, context_batch)\n",
    "        loss = -log_probs.mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    if (e % 10) == 0:\n",
    "        print(\"epoch {}: loss {}\".format(e, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample from the posterior for one context\n",
    "context_gt = torch.ones(1, 1)\n",
    "posterior_samples = flow.sample(500, context_gt).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the pdf on a grid\n",
    "x, y = torch.meshgrid(\n",
    "    torch.arange(-4, 4, 0.01), torch.arange(-3, 3, 0.01), indexing=\"xy\"\n",
    ")\n",
    "xy = torch.stack((x.flatten(), y.flatten()), dim=1)\n",
    "\n",
    "probs = torch.exp(flow.log_prob(xy, context_gt.repeat(xy.shape[0], 1)).detach())\n",
    "probs[torch.isnan(probs)] = 0.0\n",
    "\n",
    "probs_np = probs.reshape(x.shape).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "\n",
    "plot_posterior_samples = False\n",
    "plot_posterior_pdf = True\n",
    "\n",
    "plt.plot(\n",
    "    X[context.squeeze() == 0, 0],\n",
    "    X[context.squeeze() == 0, 1],\n",
    "    \".\",\n",
    "    alpha=0.4,\n",
    "    label=\"context 0\",\n",
    ")\n",
    "plt.plot(\n",
    "    X[context.squeeze() == 1, 0],\n",
    "    X[context.squeeze() == 1, 1],\n",
    "    \".\",\n",
    "    alpha=0.4,\n",
    "    label=\"context 1\",\n",
    ")\n",
    "\n",
    "if plot_posterior_samples:\n",
    "    plt.plot(\n",
    "        posterior_samples[:, 0],\n",
    "        posterior_samples[:, 1],\n",
    "        \".\",\n",
    "        label=f\"Posterior Samples for context {context_gt.item()}\",\n",
    "        color=\"red\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "if plot_posterior_pdf:\n",
    "    plt.pcolormesh(x[0,], y[:, 0], probs_np[:-1, :-1], label=\"posterior pdf\")\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-3, 3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_posterior_pdf:\n",
    "    plt.pcolormesh(x[0,], y[:, 0], probs_np[:-1, :-1], label=\"posterior pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets test this with different shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10_000\n",
    "\n",
    "X, ids, shift = get_moons(n, mode=\"random\")\n",
    "context = torch.cat([ids.T, shift.T]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cretae a dataset\n",
    "dataset = data.TensorDataset(X, context)\n",
    "# define a dataloader\n",
    "train_loader = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard optimizer used across a lot of neural network tasks\n",
    "\n",
    "input_dim = 2\n",
    "context_dim = 3\n",
    "bijector_dim = 2\n",
    "\n",
    "flow = NormalizingFlow(\n",
    "    input_dim, context_dim, bijector_dim, affine_bijector, num_layers=5\n",
    ")\n",
    "opt = optim.Adam(flow.parameters(), lr=0.001)\n",
    "\n",
    "# training loop\n",
    "for e in range(100):\n",
    "    for x_batch, context_batch in train_loader:\n",
    "        opt.zero_grad()\n",
    "        log_probs = flow.log_prob(x_batch, context_batch)\n",
    "        loss = -log_probs.mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    if (e % 10) == 0:\n",
    "        print(\"epoch {}: loss {}\".format(e, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define one observation\n",
    "# id and shift\n",
    "id_gt = 0\n",
    "context_gt = torch.tensor([id_gt, -2, 5]).unsqueeze(0)\n",
    "\n",
    "# sample from posterior\n",
    "posterior_samples = flow.sample(10000, context_gt).detach()\n",
    "\n",
    "# get ground truth samples with fixed context\n",
    "\n",
    "# put just [0,0] into the moon id which is not needed\n",
    "n = 1000\n",
    "if id_gt == 0:\n",
    "    shift = torch.tensor(\n",
    "        [[context_gt.squeeze()[1].item(), context_gt.squeeze()[2].item()], [0.0, 0.0]]\n",
    "    )\n",
    "elif id_gt:\n",
    "    shift = torch.tensor(\n",
    "        [[0.0, 0.0], [context_gt.squeeze()[1].item(), context_gt.squeeze()[2].item()]]\n",
    "    )\n",
    "\n",
    "X, ids, shift = get_moons(n, mode=\"fixed\", shift=shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "plt.plot(\n",
    "    X[ids.squeeze() == 0, 0],\n",
    "    X[ids.squeeze() == 0, 1],\n",
    "    \".\",\n",
    "    alpha=0.5,\n",
    "    label=\"context 0\",\n",
    ")\n",
    "plt.plot(\n",
    "    X[ids.squeeze() == 1, 0],\n",
    "    X[ids.squeeze() == 1, 1],\n",
    "    \".\",\n",
    "    alpha=0.5,\n",
    "    label=\"context 1\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    posterior_samples[:, 0],\n",
    "    posterior_samples[:, 1],\n",
    "    \".\",\n",
    "    label=f\"Posterior Samples for context {context_gt}\",\n",
    "    color=\"red\",\n",
    "    alpha=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 8)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bonus question:*\n",
    "\n",
    "Can you come up with an example 2d dataset, for which the normalizing flow fails or has difficulties? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rvtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
